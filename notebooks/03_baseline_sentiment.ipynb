{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b22105a",
   "metadata": {},
   "source": [
    "Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60adbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 (code)\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Download punkt if necessary (for sent_tokenize)\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be9894",
   "metadata": {},
   "source": [
    "Load Frozen FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 (code)\n",
    "\n",
    "# 3.1 Choose device (T4 GPU in Colab if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)  # Should print “cuda” when GPU is attached\n",
    "\n",
    "# 3.2 Load FinBERT model + tokenizer, move model to GPU\n",
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "model.eval()  # No gradient, inference‐only\n",
    "\n",
    "# 3.3 Label mapping for FinBERT\n",
    "label_map = {0: \"NEG\", 1: \"NEU\", 2: \"POS\"}\n",
    "\n",
    "# 3.4 Helper function: run one‐chunk inference on GPU\n",
    "def frozen_sentiment_predict(text: str):\n",
    "    \"\"\"\n",
    "    Tokenize `text`, push inputs to GPU, run FinBERT, return (label, confidence).\n",
    "    \"\"\"\n",
    "    # Tokenize + pad/truncate → return PyTorch tensors\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "        probs = torch.softmax(out.logits, dim=-1).squeeze().cpu().numpy()\n",
    "    label_id = int(np.argmax(probs))\n",
    "    return label_map[label_id], float(probs[label_id])\n",
    "\n",
    "# Quick test to confirm GPU inference is working:\n",
    "test_label, test_conf = frozen_sentiment_predict(\"The market is great today.\")\n",
    "print(f\"Test → {test_label} (confidence={test_conf:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47358245",
   "metadata": {},
   "source": [
    "Chunk & Predict for All Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 (code)\n",
    "\n",
    "# # 4.1 Ensure the sampled folder exists (should already exist from Cell 1)\n",
    "# os.makedirs(\"data/sampled\", exist_ok=True)\n",
    "\n",
    "# 4.2 Define input/output paths for chunk‐level JSONL\n",
    "INPUT_PATH = \"../data/sampled/10k_sample.jsonl\"\n",
    "CHUNK_OUT  = \"../data/sampled/chunk_sentiments_frozen.jsonl\"\n",
    "\n",
    "# 4.3 Iterate over each article, split into 3‐sentence chunks, run GPU inference\n",
    "with open(INPUT_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fin, \\\n",
    "     open(CHUNK_OUT, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    for line in tqdm(fin, total=10000):\n",
    "        art = json.loads(line)\n",
    "        art_id = art.get(\"id\", None)\n",
    "        full_text = art.get(\"headline_summary\", \"\") + \" \" + art.get(\"body\", \"\")\n",
    "\n",
    "        # 4.3.1 Sentence‐split (CPU)\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        sents = sent_tokenize(full_text)\n",
    "\n",
    "        # 4.3.2 Build 3‐sentence chunks\n",
    "        chunks = []\n",
    "        for i in range(0, len(sents), 3):\n",
    "            chunk = \" \".join(sents[i : i + 3]).strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "\n",
    "        # 4.3.3 Predict sentiment for each chunk (on GPU)\n",
    "        chunk_results = []\n",
    "        for chunk in chunks:\n",
    "            lab, conf = frozen_sentiment_predict(chunk)\n",
    "            chunk_results.append({\"text\": chunk, \"label\": lab, \"confidence\": conf})\n",
    "\n",
    "        # 4.3.4 Write out one JSONL line per article\n",
    "        fout.write(json.dumps({\n",
    "            \"article_id\": art_id,\n",
    "            \"chunks\":     chunk_results\n",
    "        }) + \"\\n\")\n",
    "\n",
    "print(\"✔ Saved chunk‐level sentiments to\", CHUNK_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af22028",
   "metadata": {},
   "source": [
    "Aggregate Chunk‐Level to Article‐Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 (code)\n",
    "\n",
    "def aggregate_article_sentiment(chunk_results):\n",
    "    \"\"\"\n",
    "    Given a list of chunk‐level dicts (each with 'label' and 'confidence'),\n",
    "    compute one aggregated label & confidence for the article.\n",
    "    \"\"\"\n",
    "    # Count how many times each label appears:\n",
    "    labels = [c[\"label\"] for c in chunk_results]\n",
    "    counts = Counter(labels)\n",
    "    # Choose the majority label; if tie, pick the one with higher average confidence\n",
    "    top_label, _ = counts.most_common(1)[0]\n",
    "    \n",
    "    # Gather confidences only for chunks with top_label\n",
    "    rel_confs = [c[\"confidence\"] for c in chunk_results if c[\"label\"] == top_label]\n",
    "    article_conf = float(np.mean(rel_confs)) if rel_confs else 0.0\n",
    "    return top_label, article_conf\n",
    "\n",
    "# 5.1 Define aggregation input/output paths\n",
    "AGG_IN  = \"../data/sampled/chunk_sentiments_frozen.jsonl\"\n",
    "AGG_OUT = \"../data/sampled/article_sentiments_frozen.csv\"\n",
    "\n",
    "rows = []\n",
    "with open(AGG_IN, \"r\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        rec = json.loads(line)\n",
    "        art_id = rec[\"article_id\"]\n",
    "        art_lbl, art_conf = aggregate_article_sentiment(rec[\"chunks\"])\n",
    "        rows.append({\"article_id\": art_id, \"label\": art_lbl, \"confidence\": art_conf})\n",
    "\n",
    "pd.DataFrame(rows).to_csv(AGG_OUT, index=False, encoding=\"utf-8\")\n",
    "print(\"✔ Saved article‐level sentiments to\", AGG_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd54374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 (code)\n",
    "\n",
    "df = pd.read_csv(\"../data/sampled/article_sentiments_frozen.csv\")\n",
    "print(\"Label distribution:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\nConfidence stats:\")\n",
    "print(df[\"confidence\"].describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
