{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb655f0",
   "metadata": {},
   "source": [
    "Imports & NLTK Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3a0c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roee1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Download the punkt tokenizer if needed\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd05b25",
   "metadata": {},
   "source": [
    "Define the Chunking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51819985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(text: str, chunk_size: int = 3):\n",
    "    \"\"\"\n",
    "    Split `text` into non‐overlapping chunks of `chunk_size` sentences.\n",
    "    Returns a list of chunk strings.\n",
    "    \"\"\"\n",
    "    sents = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(sents), chunk_size):\n",
    "        chunk = \" \".join(sents[i : i + chunk_size]).strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f2bf4",
   "metadata": {},
   "source": [
    "Quick Test on 5 Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1ea307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1 → 14 chunks; first chunk preview:\n",
      "\n",
      "Agilent Technologies A is leaving no stone unturned to bolster its Diagnostics and Genomics Group (DGG) segment on the back of its strategic partnerships. This is evident from its latest distribution ...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Article 2 → 14 chunks; first chunk preview:\n",
      "\n",
      "Ziff Davis ZD reported adjusted earnings of $1.58 per share in third-quarter 2022, which met the Zacks Consensus Estimate and increased 6% year over year. Revenues totaled $341.9 million in the quarte...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Article 3 → 12 chunks; first chunk preview:\n",
      "\n",
      "In the latest market close, Alcoa (AA) reached $26.86, with a +1.59% movement compared to the previous day. The stock exceeded the S&P 500, which registered a gain of 0.38% for the day. On the other h...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Article 4 → 10 chunks; first chunk preview:\n",
      "\n",
      "It’s the unofficial start to Q3 earnings season with the morning releases of some of the biggest banks on Wall Street. In truth, we’ve been seeing Q3 reports come in dribs and drabs previously this we...\n",
      "\n",
      "------------------------------------------------------------\n",
      "Article 5 → 13 chunks; first chunk preview:\n",
      "\n",
      "In the latest trading session, Alcoa (AA) closed at $45.50, marking a -1.45% move from the previous day. This move lagged the S&P 500's daily loss of 0.61%. Meanwhile, the Dow lost 0.42%, and the Nasd...\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test chunking on the first 5 articles from the 10k sample\n",
    "sample_chunks = []\n",
    "with open(\"../data/sampled/10k_sample.jsonl\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as fin:\n",
    "    for _ in range(5):\n",
    "        art = json.loads(next(fin))\n",
    "        full_text = art.get(\"headline_summary\", \"\") + \" \" + art.get(\"body\", \"\")\n",
    "        sample_chunks.append(chunk_by_sentences(full_text))\n",
    "\n",
    "for i, chunks in enumerate(sample_chunks, start=1):\n",
    "    print(f\"Article {i} → {len(chunks)} chunks; first chunk preview:\\n\")\n",
    "    print(chunks[0][:200] + \"...\\n\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9008f74",
   "metadata": {},
   "source": [
    "Process All 10 000 Articles & Save Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871537ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:07<00:00, 1298.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average #chunks/article: 12.8712\n",
      "Std dev of #chunks/article: 18.854764935582665\n",
      "✔ Saved chunk counts to data/sampled/chunks_per_article.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure the data/sampled folder exists\n",
    "os.makedirs(\"../data/sampled\", exist_ok=True)\n",
    "\n",
    "counts = []\n",
    "with open(\"../data/sampled/10k_sample.jsonl\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as fin:\n",
    "    for line in tqdm(fin, total=10000):\n",
    "        art = json.loads(line)\n",
    "        full_text = art.get(\"headline_summary\", \"\") + \" \" + art.get(\"body\", \"\")\n",
    "        counts.append(len(chunk_by_sentences(full_text)))\n",
    "\n",
    "# Compute and display summary stats\n",
    "counts_series = pd.Series(counts)\n",
    "print(\"Average #chunks/article:\", counts_series.mean())\n",
    "print(\"Std dev of #chunks/article:\", counts_series.std())\n",
    "\n",
    "# Save to CSV\n",
    "counts_series.to_csv(\"../data/sampled/chunks_per_article.csv\", index=False)\n",
    "print(\"✔ Saved chunk counts to data/sampled/chunks_per_article.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
